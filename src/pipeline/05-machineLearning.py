########################################################################## Function: getCostFunctions # Author: Diego Bueno - d.bueno.da.silva.10@student.scu.edu.au# Date: 25/04/2021# Description: getCostFunctions returns a dataframe with Degree, #             MSE and nFolds for each degree of rangeDegrees# # Parameters:         #            _x: array of features values#            _y: array of labeled values #            rangeDegrees: range of desired degree to be evaluated# # Return: costFunctionResults: Pandas dataframe with 'Degree', 'MSE', 'nFolds' columns###########################################################################def getCostFunctions(_x, _y, rangeDegrees, _nFolds):    #Constants    _RANGE_OF_POLY_DEGREE = 10 ## tested up to 100    _N_MAX_FOLDS_TRAINING_SET = 20 ## it takes 2 min to train 100 different kFold    _N_FOLDS_TEST_SET = 2        # Defining to split and to shuffle the Training-set into _nFolds consecutive folds     kf = KFold(n_splits = _nFolds, shuffle = True, random_state = 42)        # Create a DataFrame to save MSE results    costFunctionResults = pd.DataFrame([], columns = ['Degree', 'MSE', 'nFolds'])        # Evaluating polynomial degree according to defined rangeDegrees  ( 1 to include linear model)    for i in rangeDegrees:                # Create the poly model in each i degree        poly = PolynomialFeatures( degree = i )                # Set up the model with array _x and instance of Regression        X_poly = poly.fit_transform(_x)         poly_model = LinearRegression()                # Calculating the MSE with KF scoring         # see scoring options at:         # https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values        mse = cross_val_score(poly_model, X_poly, _y, cv=kf,  scoring='neg_mean_squared_error')*(-1)                # Salving values into costFunctionResults dataframe        df = pd.DataFrame([[i, mse.mean(), _nFolds]], columns = ['Degree', 'MSE', 'nFolds'])        costFunctionResults = costFunctionResults.append( df , ignore_index = True )                   return(costFunctionResults)               """ End of getCostFunctions """ """ Function plotErrosByDegree(_df)    plotErrosByDegree plots a graph with comparisson between Errors and Degrees        parameters:                _df: dataframe with erros values and degrees        _typeError: Type of error to plot.(_MSE, _RMSE)    """def plotErrosByDegree(_df):            lenDf = len(_df.index)            # Define llenDf colours to illustrate the error result per degree        jet = plt.get_cmap('jet')        colors = iter(jet(np.linspace(0,1,lenDf - 1)))            for index, row in _df.iterrows():                if index > 1: # do not plot linear regression                # Plotting values per error                plt.scatter(row["Degree"],row["MSE"], color= next(colors) )                plt.xlabel("Polynomial degree: up to " + str(lenDf)+ " degrees")                 plt.ylabel("Cost Function by MSE" )            # show the plot of Polynomial degree vs Cost Function by MSE        plt.show()        """ End of plotErrosByDegree """ """ Function getTheBestDegree(_d)    getTheBestDegree returns the best degree and its MSE from results saved in dataframe        parameters:                df: dataframe with errors values and degrees           return:                theBestDegree: the degree with the lower MSE value        MSE: the value of MSE        nFold: the N Kfold that performes its MSE and degree    """def getTheBestDegree(df, relativeMSE):    theBestDegree = 0    MSE = 0    nFold = 0    for index, row in df.iterrows():                if index == 0:            MSE = row['MSE']            nFold = row['nFolds']                if abs((row['MSE'] - relativeMSE )) < abs(( MSE - relativeMSE )):            MSE = row['MSE']            theBestDegree = row['Degree']            nFold = row['nFolds']            return theBestDegree, MSE, nFold""" End of getTheBestDegree """""" Function getDataSet( )    Getting dataset from CSV file and import it into a Pandas dataframe     parameters: null    return:         X: Array with values of 'Parents Income' column        y: Array with values of 'Child Performance' column    """def getDataSet( ):    # Dataset file should be in the same folder of the script    # Getting the path of the file to avoid MacOS issues    path = pathlib.Path(__file__).resolve().parent    path = str(path) + "/"    dataframe = pd.read_csv( path + 'Ass1.csv')    # Saving the number of rows and colums into r and c variables    [r, c] = dataframe.shape     # Creating X array with values of 'Parents Income' column    _X = dataframe.iloc[:, 1:c-1].values    # Creating y array with values of 'Child Performance' column    _y = dataframe.iloc[:, c-1:c].values         return _X, _y""" End of getDataSet """""" Function selectTheModel( )    selectTheModel  identifies and selects the model solution.        Parameters: null        Return:                bestMSEDegree: The selected degree for polynomial model.       """           def selectTheModel( ):        """    # Splitting the total dataset into 80% for Training-set and 20% for the Test-set    #    # The 80% split is training using KFold model selection witn N folds ( N will be found soon)    #    # The 20% Test set to evaluate if the model chose is robust and avoid overfitting    #    """        X, y = getDataSet( )        [X_Training, X_Test, y_Training, y_Test] = train_test_split(X, y, test_size = 0.2, random_state= 0 )            """ Training the model with 80% of the dataset to find the best value for k fold """        """    # Finding the ideal N fold according to LOOCV technique applying in the range of 10 degrees    #    # KFold(n_splits=N) where N is the number of instance in the dataset    #    # *** For 2000 instances, it takes 1 minutes to calculate ***    #    """    kFoldsLeaveingOneOut = LeaveOneOut().get_n_splits(X_Training)    costFunctionAtLOOCV = getCostFunctions(X_Training, y_Training, range(1, 11, 1), kFoldsLeaveingOneOut )          # Finding best degree in results saved in costFunctionAtLOOCV using zero as relative MSE    bestMSEDegree, ideal_MSE, _ = getTheBestDegree(costFunctionAtLOOCV, 0)    print("\nThe best degree using \"Leaving One Out KFold\" is ", bestMSEDegree, " with MSE ", round(ideal_MSE,2))        """ Also, it is clear to evaluate the best degree on the graph """    #     # Ploting training-set errors result ( remove the comment '#' below to see the graph )    plotErrosByDegree(costFunctionAtLOOCV)        """    # Following, using the best degree obteined with LOOCV, let's find the best N for kFold     # from 2 Folds to _N_MAX_FOLDS_TRAINING_SET Folds. You may amplify the number of tested folds.    #     # *** For 2000 instances and 100 possible kFolds, it takes 2 min to calculate ***    """        # Creating lists to save degree, mean MSE, min MSE and max MSE relative to the mean MSE    means, mins, maxs = list(),list(),list()    nFolds = range(2,_N_MAX_FOLDS_TRAINING_SET + 1,1)   # Create a DataFrame to save MSE results and identify best N for KFolder    resultsForAllKFolders = pd.DataFrame([], columns = ['Degree', 'MSE', 'nFolds'])            for n in nFolds:            # get the cost Function using the best degree from LOOCV for each N possible for kFold            costFunctionTrainingDataSet = getCostFunctions(X_Training, y_Training, range(bestMSEDegree, bestMSEDegree + 1, 1), n)                  # Salving values into resultsForAllKFolders dataframe        resultsForAllKFolders = resultsForAllKFolders.append( costFunctionTrainingDataSet , ignore_index = True )                               # Salving the mean for comparisson amoung kFolds        means.append(costFunctionTrainingDataSet['MSE'].mean())                # the minimum and maximum are divided by 100 for better visualization on bar graph        mins.append(costFunctionTrainingDataSet['MSE'].min()/100)        maxs.append(costFunctionTrainingDataSet['MSE'].max()/100)       # Print the best N for K folder based on idela MSE    _, _, bestNinKFolder = getTheBestDegree(resultsForAllKFolders, ideal_MSE)    print("\nThe ideal N value in KFold for training the dataset is ", bestNinKFolder )        """    # Ploting error bars comparing with ideal MSE got from LOOCV results    #     # The red line is the MSE calculate from LOOCV    #     # The bars represent the variation from minimum and maximum MSE    #    # The plots represents the mean MSE        """    # plot the error bar with mean, min and max    plt.errorbar(nFolds, means, yerr=[mins, maxs], fmt='o')        # plot the ideal case in red color    plt.plot(nFolds, [ideal_MSE for _ in range(len(nFolds))], color='red')        plt.xlabel("Number of K Folds")    plt.ylabel("Cost Function by MSE")                """ WHAT IS FOUND:    #    # As shown on the plotted graph, split the dataset into 15 Folds are ideal as it is    # splitting the dataset into 2000 Folds (LeaveOneOut)    #    # The best degree idenfified on costFunctionAtLOOCV is 4    #    # Thus, let's evaluate the model on the test-set (20% remainder) with KFold 2     # and evaluate if degree 4 is really the best option.    #    """    # Getting MSE values for degree from 1 to 10 using Test set and only 2 Folds    costFunctionTestSet = getCostFunctions(X_Test, y_Test, range(1,11,1), _N_FOLDS_TEST_SET)          # Curiously, comparing the mean MSE of all degrees of test set, match with best MSE/degree of training                 plt.scatter([15], costFunctionTrainingDataSet['MSE'].mean(), color='green')        # show the plot of Cost Function by MSE vs Number of K Folds    plt.show()        print("\nThe mean MSE on degrees 1 to 10 of Test-set is  ", round(costFunctionTrainingDataSet['MSE'].mean(),2))    #print(costFunctionTestSet)         """              # Comparing the difference amount Traing-set (LeaveOneOut) and Test-set for all degrees    """        df = pd.merge(costFunctionAtLOOCV, costFunctionTestSet, how='left', on='Degree', suffixes=('_Training', '_Test'))        # Plotting MSE comparision for each degree        plt.title('Comparision of MSE betwenn Training-set and Test-set by Degree')    plt.figlegend(labels=('Training','Test'),loc='upper left')          print("\nThe differences by degree between Traing-set and Test-set are:\n")    for index, row in df.iterrows():        if index > 0: # Do not plot linear for better visualization            print("\nDegree ", row["Degree"], ": ", round(row["MSE_Test"] - row["MSE_Training"],2)  )            plt.scatter(row["Degree"], row["MSE_Training"], color = "blue")            plt.scatter(row["Degree"], row["MSE_Test"], color = "red")    plt.xlabel("Degree")        plt.ylabel("Cost Functions by MSE")           # show the plot    plt.show()            """  IN CONLUSION:            Even though the difference between Training-set MSE and Test-set MSE is lower        on degree 5 than degree 4, the values are very nearby to each other in comparison        with the remainder of degrees.                Considering that all dataset is small and there are no enough data to split into an ideal        Test-set, degree 4 with 15 Folds is chosen as a selected model for inferring how         the income of parents affects the performance of children at school.        """            return(bestMSEDegree)"""    Function fit_model1()        fit_model1 deploys the solution with defined degree and Kfolder.        Parameters: null        Return:        fittedRegression: The fitted regression object that contains the trained model.     """def fit_model1():    # Getting the arrays X and y from dataset.        X, y = getDataSet( )        # Get the selected model    degree = selectTheModel()                     # Create the poly model with degree 4    poly = PolynomialFeatures(degree = degree)        # Set up the model with feature X according to degree 4    X_poly = poly.fit_transform(X)    # Create the model polynomial degree 4 according to X_poly feature and y label    fittedRegression = LinearRegression().fit(X_poly, y)        return(fittedRegression)