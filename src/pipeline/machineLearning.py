import pandas as pdimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.linear_model import LinearRegressionfrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.model_selection import KFoldfrom sklearn.model_selection import cross_val_scorefrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import LeaveOneOut""" Machine Learning hyper-parameters """ _RANGE_OF_POLY_DEGREE = 5 _N_MAX_FOLDS_TRAINING_SET = 10 _N_FOLDS_TEST_SET = 2########################################################################## Function: getMachineLearningModel # Author: Diego Bueno - d.bueno.da.silva.10@student.scu.edu.au# Date: 18/04/2021# Description: getMachineLearningModel returns the Machine Learning model #              with the best degree and Kfolder for the given dataframe.# # Parameters:#        dataframe: dataframe#        posX: X index on the dataframe#        posY: Y index on the dataframe#        # Return:  #        fittedRegression: The fitted regression object that contains #                          the trained model.     ###########################################################################def getMachineLearningModel(MyDataFrame, posX, posY):    # Getting the arrays X and y from dataset.        X, y =  getXandY( MyDataFrame, posX, posY)#    print(X)    #    print(y)        # Get the selected model    degree = selectTheModel(X, y)                     # Create the poly model with the best degree    poly = PolynomialFeatures(degree = degree)        # Set up the model with feature X according to the best degree    X_poly = poly.fit_transform(X)    # Create the model polynomial with choice degree according to X_poly feature and y label    fittedRegression = LinearRegression().fit(X_poly, y)        print("for this dataframe is defined regression model of degree", degree)        return(fittedRegression, degree)########################################################################## Function: getCostFunctions # Author: Diego Bueno - d.bueno.da.silva.10@student.scu.edu.au# Date: 18/04/2021# Description: getCostFunctions returns a dataframe with Degree, #             MSE and nFolds for each degree of rangeDegrees# # Parameters:         #            _x: array of features values#            _y: array of labeled values #            rangeDegrees: range of desired degree to be evaluated# # Return: costFunctionResults: Pandas dataframe with 'Degree', 'MSE', 'nFolds' columns###########################################################################def getCostFunctions(_x, _y, rangeDegrees, _nFolds):    print(".")        # Defining to split and to shuffle the Training-set into _nFolds consecutive folds     kf = KFold(n_splits = _nFolds, shuffle = True, random_state = 42)        # Create a DataFrame to save MSE results    costFunctionResults = pd.DataFrame([], columns = ['Degree', 'MSE', 'nFolds'])        # Evaluating polynomial degree according to defined rangeDegrees  ( 1 to include linear model)    for i in rangeDegrees:                print(".")                # Create the poly model in each i degree        poly = PolynomialFeatures( degree = i )                # Set up the model with array _x and instance of Regression        X_poly = poly.fit_transform(_x)         poly_model = LinearRegression()                # Calculating the MSE with KF scoring         # see scoring options at:         # https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values        mse = cross_val_score(poly_model, X_poly, _y, cv=kf,  scoring='neg_mean_squared_error')*(-1)                # Salving values into costFunctionResults dataframe        df = pd.DataFrame([[i, mse.mean(), _nFolds]], columns = ['Degree', 'MSE', 'nFolds'])        costFunctionResults = costFunctionResults.append( df , ignore_index = True )                   return(costFunctionResults)               ########################################################################## Function: plotErrosByDegree # Author: Diego Bueno - d.bueno.da.silva.10@student.scu.edu.au# Date: 18/04/2021# Description: plotErrosByDegree plots a graph with comparisson #             between Errors and Degrees# # Parameters:         #        _df: dataframe with erros values and degrees#        _typeError: Type of error to plot.(_MSE, _RMSE)# # Return: None###########################################################################def plotErrosByDegree(_df):            lenDf = len(_df.index)            # Define llenDf colours to illustrate the error result per degree        jet = plt.get_cmap('jet')        colors = iter(jet(np.linspace(0,1,lenDf - 1)))            for index, row in _df.iterrows():                if index > 1: # do not plot linear regression                # Plotting values per error                plt.scatter(row["Degree"],row["MSE"], color= next(colors) )                plt.xlabel("Polynomial degree: up to " + str(lenDf)+ " degrees")                 plt.ylabel("Cost Function by MSE" )            # show the plot of Polynomial degree vs Cost Function by MSE        plt.show()                return None            ########################################################################## Function: getTheBestDegree # Author: Diego Bueno - d.bueno.da.silva.10@student.scu.edu.au# Date: 18/04/2021# Description: getTheBestDegree returns the best degree and its MSE #             from results saved in dataframe.# # Parameters:         #        df: dataframe with errors values and degrees#        # # Return:  #        theBestDegree: the degree with the lower MSE value#        MSE: the value of MSE#        nFold: the N Kfold that performes its MSE and degree ###########################################################################def getTheBestDegree(df, relativeMSE):    theBestDegree = 0    MSE = 0    nFold = 0    for index, row in df.iterrows():                if index == 0:            MSE = row['MSE']            nFold = row['nFolds']                if abs((row['MSE'] - relativeMSE )) < abs(( MSE - relativeMSE )):            MSE = row['MSE']            theBestDegree = row['Degree']            nFold = row['nFolds']            return theBestDegree, MSE, nFold ########################################################################## Function: getXandY # Author: Diego Bueno - d.bueno.da.silva.10@student.scu.edu.au# Date: 18/04/2021# Description: Getting X and Y from dataset a Pandas dataframe # # Parameters:         #        dataframe: dataframe#        posX: X index on the dataframe#        posY: Y index on the dataframe#        # Return:  #        X: Array with values of 'independent' variable#        y: Array with values of 'dependent' variable###########################################################################def getXandY(dataframe, posX, posY):    # Saving the number of rows and colums into r and c variables    [r, c] = dataframe.shape     # Creating X array with values of 'Parents Income' column    _X = dataframe.iloc[:, posX:c-posX-1].values    # Creating y array with values of 'Child Performance' column    _y = dataframe.iloc[:, c-posY:c].values         return _X, _y########################################################################## Function: selectTheModel # Author: Diego Bueno - d.bueno.da.silva.10@student.scu.edu.au# Date: 18/04/2021# Description: selectTheModel identifies and selects the model solution.# # Parameters:         #        X: Indenpendent variable#        Y: Labels#        # Return:  #        bestMSEDegree: The selected degree for polynomial model.       ###########################################################################def selectTheModel(X, y):        """    # Splitting the total dataset into 80% for Training-set and 20% for the Test-set    #    # The 80% split is training using KFold model selection witn N folds ( N will be found soon)    #    # The 20% Test set to evaluate if the model chose is robust and avoid overfitting    #    """        [X_Training, X_Test, y_Training, y_Test] = train_test_split(X, y, test_size = 0.2, random_state= 0 )            """ Training the model with 80% of the dataset to find the best value for k fold """        """    # Finding the ideal N fold according to LOOCV technique applying in the range of 10 degrees    #    # KFold(n_splits=N) where N is the number of instance in the dataset    #    # *** For each 1000 instances, it takes around 30 seconds to calculate ***    #    """    kFoldsLeaveingOneOut = LeaveOneOut().get_n_splits(X_Training)    costFunctionAtLOOCV = getCostFunctions(X_Training, y_Training, range(1, _RANGE_OF_POLY_DEGREE + 1, 1), kFoldsLeaveingOneOut )          # Finding best degree in results saved in costFunctionAtLOOCV using zero as relative MSE    bestMSEDegree, ideal_MSE, _ = getTheBestDegree(costFunctionAtLOOCV, 0)    #print("\nThe best degree using \"Leaving One Out KFold\" is ", bestMSEDegree, " with MSE ", round(ideal_MSE,2))          return(bestMSEDegree)